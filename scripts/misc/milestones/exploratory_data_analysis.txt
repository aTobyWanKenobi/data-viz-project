The advantage of using the GDELT 2.0 dataset is the great amount of past and present visualizations and work done with this data. As a reminder, the final idea is to have a dynamic map of the world where events appear over time (over a limited timeframe, given the size of the dataset), and the behavior (articles and mentions) of sources in relation to those events is displayed through different visual artifacts.

There is not one particular visualization that sparkled the idea, as the current form of the project is the result of a baseline idea that was there even before looking into the dataset, plus the insights gained during exploratory data analysis, when the peculiarities and complex structures of the dataset and the CAMEO ontology became clearer. 

The visualizations built using the dataset were used to understand what was feasible with GDELT before dwelling deep into the data. For example, there was the idea of plotting events on a map, and works such as "Mapping The Geography Of Television News 2009-2018" (https://blog.gdeltproject.org/mapping-the-geography-of-television-news-2009-2018/) or "Mapping A World in Motion: A Daily Dashboard of Global Conflict" (https://blog.gdeltproject.org/mapping-a-world-in-motion-a-daily-dashboard-of-global-conflict/) confirmed that the GDELT dataset allows for that kind of approach. Additionally, the GDELT blog archives (https://blog.gdeltproject.org/) contains hundreds of projects related to the dataset. A quick glance over the titles and a more careful look at work potentially similar to the project was done in order to be sure that the visualization is original.

For what concerns pre-processing, the main problem related to the dataset is that the suggested interface through Google's BigQuery was not suitable for this project due to limitations with the free offers of the service. It was hence necessary to gather a dump of the data by collecting the zip archives with event and mentions updates, published by GDELT every 15 minutes. After discussion with the TAs, the decision was to choose a predefined timeframe (currently it's the week from 5 to 11 November, but that might/will change in the final viz) and download the updates related to it.

A Python3 script is responsible for downloading the zip archives, extracting the csv files and parse them using pandas. As a vanilla version of the "one week dataset" weights around 3GB, the script filters the data with pandas in order to obtain the minimal set of data necessary for the visualization (this is still a work in progress, as fields might be removed or added from the dataset depending on what will be used in the viz). The script writes the csv to json files, but divides each update of events and mentions into categories, based on the CAMEO ontology. This allows each individual file to weight below 1MB and smoothens the D3 data loading process.
